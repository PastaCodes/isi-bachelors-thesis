\documentclass[12pt,a4paper,openright,twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{disi-thesis}
\usepackage{code-lstlistings}
\usepackage{notes}
\usepackage{shortcuts}
\usepackage{acronym}
\usepackage{amsmath}
\usepackage{mathtools}

\school{\unibo}
\programme{Corso di Laurea
%[Magistrale?]
in Ingegneria e Scienze Informatiche}
\title{
%Fancy Title
Studio e applicazione del filtro di Kalman e sue varianti per il tracciamento di corpi celesti
}
\author{
%Candidate Name
Marco Buda
}
\date{\today}
\subject{
%Supervisor's course name
Metodi Numerici
}
\supervisor{Prof.ssa
%Supervisor Here
Damiana Lazzaro
}
%\cosupervisor{Dott. CoSupervisor 1}
%\morecosupervisor{Dott. CoSupervisor 2}
\session{
%I
II % TODO: verificare
}
\academicyear{
%2022-2023
2024-2025
}

% Definition of acronyms
\acrodef{IoT}{Internet of Thing}
\acrodef{vm}[VM]{Virtual Machine}


\mainlinespacing{1.241} % line spacing in mainmatter, comment to default (1)

\begin{document}

\frontmatter\frontispiece

\begin{abstract}	
Max 2000 characters, strict.
\end{abstract}

\begin{dedication} % this is optional
Optional. Max a few lines.
\end{dedication}

%----------------------------------------------------------------------------------------
\tableofcontents   
\listoffigures     % (optional) comment if empty
\lstlistoflistings % (optional) comment if empty
%----------------------------------------------------------------------------------------

\mainmatter

%----------------------------------------------------------------------------------------
\chapter{Introduzione}
\label{chap:introduction}
%----------------------------------------------------------------------------------------

Write your intro here.
\sidenote{Add sidenotes in this way. They are named after the author of the thesis}

You can use acronyms that your defined previously,
such as \ac{IoT}.
%
If you use acronyms twice,
they will be written in full only once
(indeed, you can mention the \ac{IoT} now without it being fully explained).
%
In some cases, you may need a plural form of the acronym.
%
For instance,
that you are discussing \acp{vm},
you may need both \ac{vm} and \acp{vm}.

\paragraph{Structure of the Thesis}

\note{At the end, describe the structure of the paper}

\chapter{Fondamenti teorici}

In questo capitolo si definiscono i concetti di base su cui si svilupperanno gli argomenti di questa tesi, insieme ai modelli matematici che li descrivono. Si presenta, inoltre, una costruzione algebrica del filtro di Kalman e si offre una panoramica delle sue varianti.

\section{I sistemi dinamici}

Un sistema dinamico è un qualunque sistema che evolve nel tempo secondo una determinata legge. \\

A seconda dell'obiettivo, si individuano le proprietà del sistema che interessa esaminare (es.: posizione e velocità di un oggetto), dette variabili di stato, e le si raccoglie in un vettore, generalmente $x\in\mathbb{R}^n$, andando a definire una cosiddetta rappresentazione in spazio di stato. \\
Nella forma generale, si considera il tempo come continuo ($t\in\mathbb{R}$), per cui si parla di modelli continui, espressi con equazioni differenziali:
$$\dfrac{d}{dt}\bigl(x(t)\bigr)=f\bigl(t,x(t)\bigr)$$ \pagebreak

Tuttavia, si può decidere di considerare lo stato soltanto in determinati istanti $t_k$. Ne risultano modelli discreti, su cui si concentrerà questa tesi, i quali possono essere espressi con relazioni di ricorrenza:
$$x_k=f(k,x_{k-1})$$

In genere si sceglie di esplicitare la presenza di ingressi $u\in\mathbb{R}^l$ nel sistema (\textit{control input}, es.: forza di gravità, propulsione...), assunti deterministici, e di misurazioni $z\in\mathbb{R}^m$ (\textit{measurement}), effettuate sulle variabili osservate. Dunque il modello viene espresso con le equazioni:
\begin{gather*}
x_k=f(k,x_{k-1},u_{k-1}) \\
z_k=h(k,x_k)
\end{gather*}
La prima è detta ``equazione di stato'' e la seconda ``trasformazione di uscita''. \\

Modellizzare la realtà comporta spesso che la legge di evoluzione $f$ del sistema sia non deterministica, per via di approssimazioni o fenomeni trascurati (\textit{process noise}, $w\in\mathbb{R}^n$), da cui l'importanza di effettuare misurazioni periodiche per monitorare la reale evoluzione dello stato.

Poiché le misurazioni stesse sono soggette a disturbi (\textit{measurement noise}, $v\in\mathbb{R}^m$), il modello completo diventa:
\begin{gather}
x_k=f_\text{det}(k,x_{k-1},u_{k-1})+w_{k-1} \\
z_k=h(k,x_k)+v_k
\end{gather}

Un caso particolare riguarda i modelli lineari, cioè esprimibili nella seguente forma matriciale:
\begin{gather}
x_k=A_kx_{k-1}+B_ku_{k-1}+w_{k-1} \label{eq:transition}\\
z_k=H_kx_k+v_k \label{eq:output}
\end{gather}
In questo caso, $A_k\in\mathbb{R}^{n\times n}$ è detta ``matrice dinamica'' e determina l'evoluzione dello stato dal passo $k-1$ al passo $k$ in assenza di ingressi e disturbi, $B_k\in\mathbb{R}^{l\times n}$ è detta ``matrice di ingresso'' e determina il contributo degli ingressi che agiscono dal passo $k-1$ al passo $k$ e $H_k\in\mathbb{R}^{m\times n}$ è detta ``matrice di uscita'' e determina le variabili osservate in base alle variabili da stimare. \\

In generale, i modelli sono tempo-varianti (o non stazionari), da cui $k$ al pedice di $A_k$, $B_k$ e $H_k$, ma si evidenziano i modelli LTI, ossia lineari tempo-invarianti (o lineari stazionari) in cui le matrici $A$, $B$ e $H$ sono costanti nel tempo. \\

Dalla combinazione delle imprecisioni nel modello e nelle misurazioni, nasce la necessità di algoritmi che raccolgano ed interpretino i dati osservati per determinare una stima (\textit{state estimate}, $\hat{x}$) dello stato reale (\textit{ground truth}, $x$). Questi algoritmi sono detti, appunto, stimatori.

\section{Il filtro di Kalman}

Il filtro di Kalman~\cite{10.1115/1.3662552}~\cite{WelchB95}~\cite{10.1145/3363294} è uno stimatore lineare ricorsivo che minimizza l'errore quadratico medio. \\
L'aspetto ricorsivo implica, innanzitutto, che si tratta di un filtro a tempo discreto e si rifà al fatto che ogni stima $\hat{x}_k$ è determinata in base alla stima precedente $\hat{x}_{k-1}$ e alla misurazione attuale $z_k$, senza richiedere l'utilizzo esplicito di $\hat{x}_0,\hat{x}_1,...,\hat{x}_{k-2}$ o $z_0,...,z_{k-1}$. Si tratta di uno dei principali vantaggi del filtro, in quanto riduce sia la complessità temporale che quella spaziale, senza compromettere l'ottimalità. \\
Come anticipato, l'ottimalità è definita dal fatto che, ad ogni passo $k$, l'algoritmo produce la stima $\hat{x}_k$ che minimizza la quantità $\mathbb{E}[\lVert e_k\rVert^2]$, con $e_k=\hat{x}_k-x_k$. \\

Il filtro è applicabile a un qualsiasi modello lineare a tempo discreto, anche tempo-variante, ossia descritto dalle equazioni \ref{eq:transition} e \ref{eq:output}.

Sono, però, posti vincoli di non correlazione sulle variabili aleatorie:
\begin{align*}
& \mathrm{Cov}(w_k,v_j)=0_{n\times m}, && \forall\,k,\forall\,j \\
& \mathrm{Cov}(w_k,u_j)=0_{n\times l}, && \forall\,k,\forall\,0\leq j\leq k-1 \\
& \mathrm{Cov}(v_k,u_j)=0_{m\times l}, && \forall\,k,\forall\,0\leq j\leq k-2 \\
& \mathrm{Cov}(w_k,w_j)=0_{n\times n}, && \forall\,k,\forall\,j\neq k \\
& \mathrm{Cov}(v_k,v_j)=0_{m\times m}, && \forall\,k,\forall\,j\neq k \\
& \mathrm{Cov}(w_k,x_0)=0_{n\times n}, && \forall\,k \\
& \mathrm{Cov}(v_k,x_0)=0_{m\times n}, && \forall\,k
\end{align*}

Infine, nelle formulazioni standard del filtro, si richiede che $w$ e $v$ siano privi di \textit{bias}, ossia che $\mathbb{E}[w_k]=\underline{0}$ e $\mathbb{E}[v_k]=\underline{0}$, ad ogni passo $k$.

\section{Costruzione del filtro lineare}

Nella letteratura sono riportate diverse derivazioni e dimostrazioni di ottimalità del filtro di Kalman~\cite[pp.~107-113]{10.5555/2823801}~\cite{10.48550/arXiv.1910.03558}. Si presenta qui una costruzione algebrica il più elementare possibile. \\

Il primo passo è descrivere l'espressione che calcoli la stima $\hat{x}_k$. Al passo $k\geq 1$ sono disponibili le informazioni riguardo $\hat{x}_{k-1}$, $u_{k-1}$ e $z_k$, per cui l'espressione lineare avrà la forma generica:
\begin{equation} \label{eq:generic-estimate}
\hat{x}_k=\mathcal{A}_k\hat{x}_{k-1}+\mathcal{B}_ku_{k-1}+\mathcal{K}_kz_k
\end{equation}
Per quanto riguarda il passo $k=0$, la scelta della stima iniziale $\hat{x}_0$ è libera, purché sia deterministica. Se si hanno informazioni sulla distribuzione di $x_0$, è consigliabile scegliere $\hat{x}_0=\mathbb{E}[x_0]$. \\

Una condizione implicita sul filtro richiede che anche l'errore sulle stime generate $e_k$ sia privo di \textit{bias}, ossia, ad ogni passo $k$:
$$\mathbb{E}[e_k]=\mathbb{E}[\hat{x}_k-x_k]=\underline{0}$$
Da questa condizione si otterranno informazioni sulle matrici incognite $\mathcal{A}$, $\mathcal{B}$ e $\mathcal{K}$. In effetti, considerando per $k\geq 1$ e sostituendo $\hat{x}_k$ con la sua definizione (\ref{eq:generic-estimate}), si ottiene:
$$\mathbb{E}[\mathcal{A}_k\hat{x}_{k-1}+\mathcal{B}_ku_{k-1}+\mathcal{K}_kz_k-x_k]=\underline{0}$$
Successivamente, sostituendo $z_k$ con la sua definizione (\ref{eq:output}):
$$\mathbb{E}\left[\mathcal{A}_k\hat{x}_{k-1}+\mathcal{B}_ku_{k-1}+\mathcal{K}_k\bigl(H_kx_k+v_k\bigr)-x_k\right]=\underline{0}$$
Sostituendo $x_k$ con la relazione di ricorrenza (\ref{eq:transition}) e manipolando i termini:
\begin{gather*}
\begin{multlined}[\textwidth]
\mathbb{E}\Bigl[\mathcal{A}_k\hat{x}_{k-1}+\mathcal{B}_ku_{k-1}+\mathcal{K}_k\Bigl(H_k\bigl(A_kx_{k-1}+B_ku_{k-1}+w_{k-1}\bigr)+v_k\Bigr)+ \\
-\bigl(A_kx_{k-1}+B_ku_{k-1}+w_{k-1}\bigr)\Bigr]=\underline{0}
\end{multlined} \\
\begin{multlined}[\textwidth]
\Rightarrow\mathbb{E}\bigl[\mathcal{A}_k\hat{x}_{k-1}+\mathcal{B}_ku_{k-1}+\mathcal{K}_kH_kA_kx_{k-1}+\mathcal{K}_kH_kB_ku_{k-1}+\mathcal{K}_kH_kw_{k-1}+ \\
+\mathcal{K}_kv_k-A_kx_{k-1}-B_ku_{k-1}-w_{k-1}\bigr]=\underline{0}
\end{multlined} \\
\begin{multlined}[\textwidth]
\Rightarrow\mathbb{E}\bigl[\mathcal{A}_k\hat{x}_{k-1}+\mathcal{B}_ku_{k-1}+\mathcal{K}_kH_kA_kx_{k-1}+\mathcal{K}_kH_kB_ku_{k-1}+\mathcal{K}_kH_kw_{k-1}+ \\
+\mathcal{K}_kv_k-A_kx_{k-1}-B_ku_{k-1}-w_{k-1}-\mathcal{A}_kx_{k-1}+\mathcal{A}_kx_{k-1}\bigr]=\underline{0}
\end{multlined} \\
\begin{multlined}[\textwidth]
\Rightarrow\mathbb{E}\bigl[\mathcal{A}_k\bigl(\hat{x}_{k-1}-x_{k-1}\bigr)+\bigl(\mathcal{K}_kH_kA_k-A_k+\mathcal{A}_k\bigr)x_{k-1}+ \\
+\bigl(\mathcal{K}_kH_kB_k-B_k+\mathcal{B}_k\bigr)u_{k-1}+\bigl(\mathcal{K}_kH_k-I\bigr)w_{k-1}+\mathcal{K}_kv_k\bigr]=\underline{0}
\end{multlined} \\
\begin{multlined}[\textwidth]
\Rightarrow\mathcal{A}_k\mathbb{E}[\hat{x}_{k-1}-x_{k-1}]+\bigl(\mathcal{K}_kH_kA_k-A_k+\mathcal{A}_k\bigr)\mathbb{E}[x_{k-1}]+ \\
+\bigl(\mathcal{K}_kH_kB_k-B_k+\mathcal{B}_k\bigr)\mathbb{E}[u_{k-1}]+\bigl(\mathcal{K}_kH_k-I\bigr)\mathbb{E}[w_{k-1}]+\mathcal{K}_k\mathbb{E}[v_k]=\underline{0}
\end{multlined}
\end{gather*}
Sfruttando l'ipotesi che le quantità $\hat{x}_{k-1}-x_{k-1}$, $w_{k-1}$ e $v_k$ siano prive di \textit{bias}:
$$\bigl(\mathcal{K}_kH_kA_k-A_k+\mathcal{A}_k\bigr)\mathbb{E}[x_{k-1}]+\bigl(\mathcal{K}_kH_kB_k-B_k+\mathcal{B}_k\bigr)\mathbb{E}[u_{k-1}]=\underline{0}$$
Non potendo fare assunzioni su $x$ e $u$, ne segue che:
$$\mathcal{K}_kH_kA_k-A_k+\mathcal{A}_k=0_{n\times n}\enspace,\quad\mathcal{K}_kH_kB_k-B_k+\mathcal{B}_k=0_{l\times n}$$
$$\Rightarrow\mathcal{A}_k=(I-\mathcal{K}_kH_k)A_k\enspace,\quad\mathcal{B}_k=(I-\mathcal{K}_kH_k)B_k$$

Risulta pratico definire la seguente quantità, per $k\geq 1$, come una stima \textit{a priori}, ossia che non tenga conto della misurazione $z_k$:
$$\hat{x}_k^-=A_k\hat{x}_{k-1}+B_ku_{k-1}$$
Così facendo, si vanno a distinguere due fasi all'interno di ogni passo: una fase di predizione, o \textit{time update}, in cui si calcola $\hat{x}_k^-$ in base alle conoscenze sul modello, e una fase di correzione, o \textit{measurement update}, in cui si combina $\hat{x}_k^-$ con la misurazione $z_k$ per ottenere una stima ottimale $\hat{x}_k$. \\
L'espressione per ricavare $\hat{x}_k$, ossia la stima \textit{a posteriori}, diventa ora:
$$\hat{x}_k=(I-\mathcal{K}_kH_k)A_k\hat{x}_{k-1}+(I-\mathcal{K}_kH_k)B_ku_{k-1}+\mathcal{K}_kz_k=$$
$$=(I-\mathcal{K}_kH_k)(A_k\hat{x}_{k-1}+B_ku_{k-1})+\mathcal{K}_kz_k=(I-\mathcal{K}_kH_k)\hat{x}_k^-+\mathcal{K}_kz_k=$$
$$=\hat{x}_k^-+\mathcal{K}_k(z_k-H_k\hat{x}_k^-)$$

Proseguendo, si definisca $e_k^-=\hat{x}_k^--x_k$ come l'errore sulla stima \textit{a priori} e si consideri:
$$e_k^-=\hat{x}_k^--x_k=(A_k\hat{x}_{k-1}+B_ku_{k-1})-(A_kx_{k-1}+B_ku_{k-1}+w_{k-1})=$$
$$=A_k(\hat{x}_{k-1}-x_{k-1})-w_{k-1}=A_ke_{k-1}-w_{k-1}$$
Si osservi che anche $e_k^-$ è privo di \textit{bias}:
$$\mathbb{E}[e_k^-]=\mathbb{E}[A_ke_{k-1}-w_{k-1}]=A_k\mathbb{E}[e_{k-1}]-E[w_{k-1}]=\underline{0}$$

Si definiscano, per $k\geq 1$, le auto-covarianze degli errori $P_k^-=\mathrm{Cov}(e_k^-,e_k^-)$ e $P_k=\mathrm{Cov}(e_k,e_k)$. \\
Si ha:
\begin{gather*}
P_k^-=\mathbb{E}\left[(e_k^--\mathbb{E}[e_k^-])(e_k^--\mathbb{E}[e_k^-])^T\right]=\mathbb{E}\left[e_k^-(e_k^-)^T\right]= \\
=\mathbb{E}\left[(A_ke_{k-1}-w_{k-1})(A_ke_{k-1}-w_{k-1})^T\right]= \\
=\mathbb{E}\left[(A_ke_{k-1}-w_{k-1})\left({e_{k-1}}^T{A_k}^T-{w_{k-1}}^T\right)\right]= \\
\begin{multlined}[\textwidth]
=A_k\mathbb{E}\left[e_{k-1}(e_{k-1})^T\right]{A_k}^T-A_k\mathbb{E}\left[e_{k-1}(w_{k-1})^T\right]+ \\
-\mathbb{E}\left[w_{k-1}(e_{k-1})^T\right]{A_k}^T+\mathbb{E}\left[w_{k-1}(w_{k-1})^T\right]=
\end{multlined} \\
\begin{multlined}[\textwidth]
=A_k\mathbb{E}\left[(e_{k-1}-\underline{0})(e_{k-1}-\underline{0})^T\right]{A_k}^T-A_k\mathbb{E}\left[(e_{k-1}-\underline{0})(w_{k-1}-\underline{0})^T\right]+ \\
-\mathbb{E}\left[(w_{k-1}-\underline{0})(e_{k-1}-\underline{0})^T\right]{A_k}^T+\mathbb{E}\left[(w_{k-1}-\underline{0})(w_{k-1}-\underline{0})^T\right]=
\end{multlined} \\
\begin{multlined}[\textwidth]
=A_k\mathbb{E}\left[(e_{k-1}-\mathbb{E}[e_{k-1}])(e_{k-1}-\mathbb{E}[e_{k-1}])^T\right]{A_k}^T+ \\
-A_k\mathbb{E}\left[(e_{k-1}-\mathbb{E}[e_{k-1}])(w_{k-1}-\mathbb{E}[w_{k-1}])^T\right]+ \\
-\mathbb{E}\left[(w_{k-1}-\mathbb{E}[w_{k-1}])(e_{k-1}-\mathbb{E}[e_{k-1}])^T\right]{A_k}^T+ \\
+\mathbb{E}\left[(w_{k-1}-\mathbb{E}[w_{k-1}])(w_{k-1}-\mathbb{E}[w_{k-1}])^T\right]=
\end{multlined} \\
\begin{multlined}[\textwidth]
=A_k\mathrm{Cov}(e_{k-1},e_{k-1}){A_k}^T-A_k\mathrm{Cov}(e_{k-1},w_{k-1})+ \\
-\mathrm{Cov}(w_{k-1},e_{k-1}){A_k}^T+\mathrm{Cov}(w_{k-1},w_{k-1})
\end{multlined}
\end{gather*}
Si osservi che i termini centrali si annullano se $e_{k-1}$ e $w_{k-1}$ sono non correlati. In effetti, analizzando ricorsivamente l'errore $e_{k-1}$, si trova che le uniche variabili aleatorie da cui esso dipende sono $u_0,...,u_{k-2}$, $w_0,...,w_{k-2}$, $v_1,...,v_{k-1}$ e $x_0$, ossia variabili con cui $w_{k-1}$ è non correlato per ipotesi. \\
Ricordando la definizione di $P_{k-1}$ e definendo $Q_{k-1}$ come la auto-covarianza di $w_{k-1}$, si trova, dunque, l'espressione:
$$P_k^-=A_kP_{k-1}{A_k}^T+Q_{k-1}$$
La scelta iniziale di $P_0$ è pressoché libera. Se si hanno informazioni sulla distribuzione di $x_0$, è consigliabile utilizzare una stima della sua auto-covarianza. In ogni caso, è necessario avere $P_0\neq 0_{n\times n}$ e semidefinita positiva. \\

Per trovare un'espressione per $P_k$, si consideri inizialmente:
\begin{gather*}
\hat{x}_k=(I-\mathcal{K}_kH_k)\hat{x}_k^-+\mathcal{K}_kz_k \\
\Rightarrow e_k=\hat{x}_k-x_k=(I-\mathcal{K}_kH_k)\hat{x}_k^-+\mathcal{K}_kz_k-x_k
\end{gather*}
Sostituendo $z_k$ con la sua definizione (\ref{eq:output}):
\begin{gather*}
e_k=(I-\mathcal{K}_kH_k)\hat{x}_k^-+\mathcal{K}_k(H_kx_k+v_k)-x_k= \\
=(I-\mathcal{K}_kH_k)\hat{x}_k^-+\mathcal{K}_kH_kx_k+\mathcal{K}_kv_k-x_k= \\
=(I-\mathcal{K}_kH_k)\hat{x}_k^--(I-\mathcal{K}_kH_k)x_k+\mathcal{K}_kv_k= \\
=(I-\mathcal{K}_kH_k)(\hat{x}_k^--x_k)+\mathcal{K}_kv_k=(I-\mathcal{K}_kH_k)e_k^-+\mathcal{K}_kv_k
\end{gather*}
Dunque, si ha:
\begin{gather*}
P_k=\mathbb{E}\left[(e_k-\mathbb{E}[e_k])(e_k-\mathbb{E}[e_k])^T\right]=\mathbb{E}\left[e_k(e_k)^T\right]= \\
=\mathbb{E}\left[\bigl((I-\mathcal{K}_kH_k)e_k^-+\mathcal{K}_kv_k\bigr)\bigl((I-\mathcal{K}_kH_k)e_k^-+\mathcal{K}_kv_k\bigr)^T\right]= \\
=\mathbb{E}\left[\bigl((I-\mathcal{K}_kH_k)e_k^-+\mathcal{K}_kv_k\bigr)\bigl((e_k^-)^T(I-\mathcal{K}_kH_k)^T+{v_k}^T{\mathcal{K}_k}^T\bigr)\right]= \\
\begin{multlined}[\textwidth]
=(I-\mathcal{K}_kH_k)\mathbb{E}\left[e_k^-(e_k^-)^T\right](I-\mathcal{K}_kH_k)^T+(I-\mathcal{K}_kH_k)\mathbb{E}\left[e_k^-(v_k)^T\right]{\mathcal{K}_k}^T+ \\
+\mathcal{K}_k\mathbb{E}\left[v_k(e_k^-)^T\right](I-\mathcal{K}_kH_k)^T+\mathcal{K}_k\mathbb{E}\left[v_k(v_k)^T\right]{\mathcal{K}_k}^T
\end{multlined}
\end{gather*}
Anche in questo caso, l'espressione si riduce alle covarianze. I termini centrali si annullano, poiché le uniche variabili aleatorie ad influenzare $e_k^-$ sono $u_0,...,u_{k-2}$, $w_0,...,w_{k-1}$, $v_1,...,v_{k-1}$ e $x_0$, ossia variabili con cui $v_k$ è non correlato per ipotesi. \\
Ricordando la definizione di $P_k^-$ e definendo $R_k$ come la auto-covarianza di $v_k$, si trova l'espressione provvisoria:
$$P_k=(I-\mathcal{K}_kH_k)P_k^-(I-\mathcal{K}_kH_k)^T+\mathcal{K}_kR_k{\mathcal{K}_k}^T$$

Si osservi che minimizzare l'errore quadratico medio $\mathbb{E}[\Vert e_k\Vert^2]$ equivale a minimizzare la traccia di $P_k$. In effetti:
$$\mathbb{E}[\Vert e_k\Vert^2]=\mathbb{E}[{e_k}^Te_k]=\mathbb{E}[\mathrm{tr}({e_k}^Te_k)]=\mathbb{E}\left[\mathrm{tr}\bigl(e_k(e_k)^T\bigr)\right]=\mathrm{tr}\left(\mathbb{E}\left[e_k(e_k)^T\right]\right)=\mathrm{tr}(P_k)$$
Dunque, si calcoli:
\begin{gather*}
\mathrm{tr}(P_k)=\mathrm{tr}\Bigl((I-\mathcal{K}_kH_k)P_k^-(I-\mathcal{K}_kH_k)^T+\mathcal{K}_kR_k{\mathcal{K}_k}^T\Bigr)= \\
=\mathrm{tr}\Bigl((I-\mathcal{K}_kH_k)P_k^-\left(I-{H_k}^T{\mathcal{K}_k}^T\right)+\mathcal{K}_kR_k{\mathcal{K}_k}^T\Bigr)= \\
=\mathrm{tr}\left(P_k^--P_k^-{H_k}^T{\mathcal{K}_k}^T-\mathcal{K}_kH_kP_k^-+\mathcal{K}_kH_kP_k^-{H_k}^T{\mathcal{K}_k}^T+\mathcal{K}_kR_k{\mathcal{K}_k}^T\right)
\end{gather*}
Essendo $P_k^-$ simmetrica, vale $P_k^-{H_k}^T{\mathcal{K}_k}^T=(P_k^-)^T{H_k}^T{\mathcal{K}_k}^T=(\mathcal{K}_kH_kP_k^-)^T$, \\
per cui:
$$\mathrm{tr}(P_k)=\mathrm{tr}\left(P_k^-\right)-2\,\mathrm{tr}\left(\mathcal{K}_kH_kP_k^-\right)+\mathrm{tr}\left(\mathcal{K}_kH_kP_k^-{H_k}^T{\mathcal{K}_k}^T\right)+\mathrm{tr}\left(\mathcal{K}_kR_k{\mathcal{K}_k}^T\right)$$
Per ricercare $\mathcal{K}_k$ che minimizzi $\mathrm{tr}(P_k)$ si ponga:
\begin{gather*}
\dfrac{\partial\,\mathrm{tr}(P_k)}{\partial\,\mathcal{K}_k}=0_{n\times n} \\
\begin{multlined}[\textwidth]
\Rightarrow \dfrac{\partial\,\mathrm{tr}\left(P_k^-\right)}{\partial\,\mathcal{K}_k}-2\,\dfrac{\partial\,\mathrm{tr}\left(\mathcal{K}_kH_kP_k^-\right)}{\partial\,\mathcal{K}_k}+\dfrac{\partial\,\mathrm{tr}\left(\mathcal{K}_kH_kP_k^-{H_k}^T{\mathcal{K}_k}^T\right)}{\partial\,\mathcal{K}_k}+ \\
+\dfrac{\partial\,\mathrm{tr}\left(\mathcal{K}_kR_k{\mathcal{K}_k}^T\right)}{\partial\,\mathcal{K}_k}=0_{n\times n}
\end{multlined}
\end{gather*}
Si osservi che $\mathrm{tr}(P_k^-)$ è costante in $K_k$, per cui il primo termine si annulla. Per calcolare i termini restanti, si utilizzino le seguenti identità, con la seconda valida se $N$ è simmetrica:
$$\dfrac{\partial\,\mathrm{tr}(MN)}{\partial M}=N^T\enspace,\quad\dfrac{\partial\,\mathrm{tr}(MNM^T)}{\partial M}=2MN$$
Si ottiene:
\begin{gather*}
-2(H_kP_k^-)^T+2\mathcal{K}_k\left(H_kP_k^-{H_k}^T\right)+2\mathcal{K}_kR_k=0_{n\times n} \\
\Rightarrow-2P_k^-{H_k}^T+2\mathcal{K}_kH_kP_k^-{H_k}^T+2\mathcal{K}_kR_k=0_{n\times n} \\
\Rightarrow\mathcal{K}_k=P_k^-{H_k}^T\left(H_kP_k^-{H_k}^T+R_k\right)^{-1}
\end{gather*}
Esaminando la matrice Hessiana di $\mathrm{tr}(P_k)$, la quale risulta essere semidefinita positiva, si può verificare che il valore trovato rappresenta un minimo globale. \\
La quantità $\mathcal{K}_k$ è detta matrice dei guadagni di Kalman (\textit{Kalman gain}) e può essere pensata come un indice di affidabilità delle misurazioni rispetto al modello teorico. In effetti, per $R_k\to 0_{m\times m}$ si ha $\mathcal{K}_k\to H^{-1}$, per cui $\hat{x}_k\to z_k$, mentre per $P_k^-\to 0_{n\times n}$ si ha $\mathcal{K}_k\to 0_{n\times m}$, per cui $\hat{x}_k\to \hat{x}_k^-$. \\

Sostituendo il valore trovato, l'espressione per $P_k$ diventa:
\begin{gather*}
P_k=P_k^--P_k^-{H_k}^T{\mathcal{K}_k}^T-\mathcal{K}_kH_kP_k^-+\mathcal{K}_kH_kP_k^-{H_k}^T{\mathcal{K}_k}^T+\mathcal{K}_kR_k{\mathcal{K}_k}^T= \\
=\left(I-\mathcal{K}_kH_k\right)P_k^--P_k^-{H_k}^T{\mathcal{K}_k}^T+\mathcal{K}_k\left(H_kP_k^-{H_k}^T+R_k\right){\mathcal{K}_k}^T= \\
\begin{multlined}[\textwidth]
=\left(I-\mathcal{K}_kH_k\right)P_k^--P_k^-{H_k}^T{\mathcal{K}_k}^T+ \\
+P_k^-{H_k}^T\underbrace{\left(H_kP_k^-{H_k}^T+R_k\right)^{-1}\left(H_kP_k^-{H_k}^T+R_k\right)}_{I}{\mathcal{K}_k}^T=
\end{multlined} \\
=\left(I-\mathcal{K}_kH_k\right)P_k^--P_k^-{H_k}^T{\mathcal{K}_k}^T+P_k^-{H_k}^T{\mathcal{K}_k}^T= \\
=\left(I-\mathcal{K}_kH_k\right)P_k^-
\end{gather*}

Riassumendo, l'algoritmo si basa sulle seguenti equazioni:
\begin{gather*}
\hat{x}_0=\mathbb{E}[x_0] \\
P_0=\mathrm{Cov}(x_0,x_0) \\[0.2\baselineskip]
\hline \\[-0.8\baselineskip]
\hat{x}_k^-=A_k\hat{x}_{k-1}+Bu_{k-1} \\
P_k^-=A_kP_{k-1}{A_k}^T+Q_{k-1} \\
\mathcal{K}_k=P_k^-{H_k}^T\left(H_kP_k^-{H_k}^T+R_k\right)^{-1} \\
\hat{x}_k=\hat{x}_k^-+\mathcal{K}_k(z_k-H_k\hat{x}_k^-) \\
P_k=\left(I-\mathcal{K}_kH_k\right)P_k^-
\end{gather*}

\section{Varianti non lineari}

Sulla base della formulazione originale di Kálmán, sono state sviluppate numerose varianti del filtro, con l'obiettivo principale di estenderne il campo di applicazione, specie a modelli non lineari. Si presenta qui un elenco delle varianti più conosciute, successivamente schematizzate in \cref{fig:variants-diagram}:
\begin{itemize}
\item KF (``Standard'' Kalman Filter)~\cite{10.1115/1.3662552}~\cite{WelchB95}: Semplice ed ottimale per sistemi lineari.
\item EKF (Extended Kalman Filter)\iffalse~\cite{smith1962}\fi~\cite{WelchB95}: Basato sulla linearizzazione di una funzione di transizione non lineare, con rischio di divergenza.
\begin{itemize}
\item EKF2 o SO-EKF (Second-Order Extended Kalman Filter)~\cite[pp.~191-192]{10.5555/2823801}
\item MEKF (Multiplicative Extended Kalman Filter)\iffalse~\cite{paulson1969}\fi~\cite{markley2004}: Specifico per la stima dell'assetto (orientamento) espresso come quaternione.
\end{itemize}
\item Filtri Sigma-Point: Accurati e robusti in sistemi altamente dinamici, con costo di esecuzione maggiore.
\begin{itemize}
\item UKF (Unscented Kalman Filter)~\cite{882463}
\item QKF (Quadrature Kalman Filter), come il GHKF (Gauss-Hermite Kalman Filter)
\item CKF (Cubature Kalman Filter)
\end{itemize}
\item Metodo Monte Carlo: Basato sull'utilizzo di campioni.
\begin{itemize}
\item EnKF (Ensemble Kalman Filter)
\item PF (Particle Filter): Efficace anche quando i disturbi hanno distribuzione non Gaussiana.
\end{itemize}
\end{itemize}

Nell'utilizzo delle varianti EKF e Sigma-Point, spesso si sceglie di memorizzare $P_k$ fattorizzata sotto forma di radice quadrata $\sqrt{P_k}$, per favorire la stabilità numerica dell'algoritmo. Si parla, in questo caso, di SRKF (Square-Root Kalman Filters). \pagebreak

\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{figures/variants-diagram.pdf}
    \caption{Diagramma di classificazione delle varianti del filtro di Kalman.}
    \label{fig:variants-diagram}
    \vspace{2\baselineskip}
\end{figure}

\section{Sviluppi recenti}

Lo studio del filtro di Kalman resta un settore vivo, a più di 60 anni dalla sua concezione. Si continuano a identificare nuovi settori di applicazione, nuove tecniche di miglioramento dell'accuratezza, della stabilità numerica e della performance computazionale. Nuove varianti, sia generiche che specializzate, si aggiungono a quelle già consolidate. \\
Iteratively Saturated Kalman Filtering~\cite{yang2025} \\
AI-Aided Kalman Filters~\cite{shlezinger2025}

\section{Limitazioni}

Sebbene oggi assuma un ruolo fondamentale nell'ingegneria aerospaziale e in diversi altri settori, il filtro di Kalman presenta alcune limitazioni intrinseche:
\begin{itemize}
\item Per sistemi non lineari con distribuzioni non Gaussiane, non è garantita, in generale, l'ottimalità.
\item Benché la conoscenza del sistema reale sia la base dell'efficacia del filtro, questa può anche costituire un limite sulla precisione, qualora modelli sufficientemente accurati non siano disponibili.
\end{itemize}

% I suggest referencing stuff as follows: \cref{fig:random-image} or \Cref{fig:random-image}

% \begin{figure}
%     \centering
%     \includegraphics[width=.8\linewidth]{figures/random-image.pdf}
%     \caption{Some random image}
%     \label{fig:random-image}
% \end{figure}

% \section{Some cool topic}

\iffalse % Discarded chapter
\chapter{Evoluzione e sviluppi recenti}

Lo studio del filtro di Kalman resta un settore vivo, a più di 60 anni dalla sua concezione. Si continuano a identificare nuovi settori di applicazione, nuove tecniche di miglioramento dell'accuratezza, della stabilità numerica e della performance computazionale. Nuove varianti, sia generiche che specializzate, si aggiungono a quelle già consolidate. In questo capitolo si presenta un riassunto della storia ed evoluzione del filtro, insieme alle missioni che ne hanno dimostrato il merito, e si riportano alcuni dei progressi più recenti, fornendo un'anteprima della direzione attuale della ricerca.

\section{Dalla guerra fredda ad oggi}

Rudolf Emil Kálmán studiò i sistemi dinamici lineari a tempo discreto già nel 1954 per la sua tesi di laurea magistrale al MIT. Nel 1958, con l'intensificarsi della corsa allo spazio, il governo federale degli Stati Uniti d'America finanziò gli studi di Kálmán e del collega Richard S. Bucy per la stima e il controllo di sistemi aerospaziali. In questo contesto, a novembre dello stesso anno, nacque l'idea di riformulare il metodo di stima ottimale dell'esistente filtro di Wiener-Kolmogorov, utilizzando una rappresentazione in spazio di stato generica e spostando il problema dal dominio delle frequenze al dominio del tempo. Il filtro ottenuto rimuoveva il requisito di un modello tempo-invariante, permetteva di esprimere il modello con equazioni più semplici da ricavare e prometteva un algoritmo più adatto all'implementazione a computer.
Nel 1960, Kálmán pubblicò la descrizione del filtro a tempo discreto~\cite{10.1115/1.3662552} e nel 1961, insieme a Bucy, il filtro a tempo continuo~\cite{10.1115/1.3658902}, chiamato oggi filtro di Kalman-Bucy.
% TODO: riprendi da Schmidt

\section{Iteratively Saturated Kalman Filtering}

\section{AI-Aided Kalman Filters}
\fi

\chapter{Modello astronomico}

\section{Coordinate celesti}

Le osservazioni raccolte dal Minor Planet Center fanno uso di una coppia di angoli $(\alpha,\delta)$ per identificare la posizione dei corpi nella sfera celeste, o, più precisamente, per identificare la direzione orientata dall'osservatore verso la posizione apparente del corpo considerato. I due angoli sono definiti con riferimento al piano equatoriale e al punto vernale all'epoca J2000.0 (mezzogiorno del 1° gennaio 2000, calendario gregoriano e tempo terrestre). Nella pratica si considerano assi orientati secondo il sistema ICRS, adottato dalla International Astronomical Union dal 1° gennaio 1998, spostando l'origine dal baricentro del sistema solare alla posizione dell'osservatore (osservazione topocentrica). In questo contesto, l'ascensione retta $\alpha\in[0,2\pi)$ rappresenta la rotazione attorno all'asse z, mentre la declinazione $\delta\in\left[-\frac{\pi}{2},\frac{\pi}{2}\right]$ rappresenta la distanza angolare dal piano xy. Dunque, indicando con $\Delta$ la distanza apparente tra l'osservatore e il corpo al momento dell'osservazione, la posizione apparente del corpo osservato, in coordinate cartesiane topocentriche, è data da:
\begin{equation}
\begin{pmatrix} x \\ y \\ z \end{pmatrix}=R_z(\alpha)\,R_y(-\delta)\begin{pmatrix} \Delta \\ 0 \\ 0 \end{pmatrix}=\begin{pmatrix} \Delta\cos(\delta)\cos(\alpha) \\ \Delta\cos(\delta)\sin(\alpha) \\ \Delta\sin(\delta) \end{pmatrix}
\end{equation}
La conversione a coordinate baricentriche richiede semplicemente di sommare la posizione dell'osservatore rispetto al baricentro del sistema solare.
\pagebreak

Si definiscono le distanze apparenti $R$ tra l'osservatore e il Sole al momento dell'osservazione e $d$ (solitamente indicata con $r$, ma da non confondersi con la distanza dal fuoco occupato dell'orbita) tra il corpo osservato e il Sole nel momento in cui la luce osservata viene riflessa sulla superficie del corpo. Si definiscono gli angoli di separazione $\theta\in[0,\pi]$, detta elongazione, tra il corpo e il Sole come appaiono all'osservatore al momento dell'osservazione e $\phi\in[0,\pi]$, detto angolo di fase, tra l'osservatore e il Sole come appaiono dal corpo nel momento in cui la luce osservata viene riflessa. Seppure si tratti di una definizione analoga, l'angolo di fase viene definito, in genere, come l'angolo tra la luce incidente e la luce riflessa verso l'osservatore, per via del suo impatto sul modello ottico descritto nella sezione \ref{sec:photometry}. \\

Si è parlato finora di quantità apparenti. Questa precisazione è necessaria per via della velocità finita della luce e ad altri fenomeni come aberrazioni, rifrazioni ed effetti relativistici. Da questo punto in poi si considereranno le quantità apparenti come coincidenti alle controparti ``reali'' (geometriche), con la consapevolezza che ciò introdurrà imprecisioni nel modello, le quali dovranno essere considerate nell'applicazione del filtro di Kalman. Queste imprecisioni sono accettabili per il tracciamento di asteroidi, ma possono diventare problematiche per oggetti più veloci.

Con questa semplificazione diventa possibile considerare il triangolo tra il corpo osservato, l'osservatore e il Sole e applicare il teorema dei seni per determinare relazioni fra gli angoli e le distanze. In particolare:
\begin{gather}
\frac{d}{\sin(\theta)}=\frac{R}{\sin(\phi)} \\
\frac{\Delta}{\sin(\pi-\theta-\phi)}=\frac{R}{\sin(\phi)}\Rightarrow\frac{\Delta}{\sin(\theta+\phi)}=\frac{R}{\sin(\phi)}
\end{gather}

\section{Fotometria e il sistema H-G}\label{sec:photometry}

L'intensità della luce osservata è espressa dalla magnitudine apparente e può essere utilizzata per stimare la distanza del corpo. Intuitivamente, questa quantità dipende dall'angolo di fase ($\phi\approx0$ indica un corpo perfettamente illuminato, mentre $\phi\approx1$ comporta che non venga riflessa luce verso l'osservatore), dalle distanze $d$ e $\Delta$ e da alcune caratteristiche fisiche del corpo. Tuttavia non esiste un modello macroscopico esatto.

Una delle prime applicazioni delle nozioni di fotometria allo studio degli asteroidi ha prodotto il modello H-G~\cite{bowell1989}~\cite{dymock2007}, usato tuttora quando non si dispone di informazioni sufficienti per impiegare modelli più precisi. Gli asteroidi vengono caratterizzati da due parametri: $H$, detta magnitudine assoluta, che equivale alla magnitudine apparente per $d=\Delta=1~\mathrm{AU}$ e $\phi=0$\quad e $G$, detto \textit{slope parameter}, che quantifica un picco di luminosità che spesso si verifica nella curva di fase vicino all'opposizione. Se si esprimono le distanze in unità astronomiche, la magnitudine apparente è descritta da:
\begin{gather}
V=H+5\log_{10}(d\Delta)-2.5\log_{10}(\Phi(\phi)) \\
\Phi(\phi)=(1-G)\exp\left(-3.33\bigl(\tan\left(\tfrac{\phi}{2}\right)\bigr)^{0.63}\right)+G\exp\left(-1.87\bigl(\tan\left(\tfrac{\phi}{2}\right)\bigr)^{1.22}\right)
\end{gather}

\section{Orbite ellittiche}

Il moto dei pianeti, degli asteroidi e delle comete è ben approssimato da orbite ellittiche, con un fuoco occupato dal baricentro del sistema solare. Tali orbite possono essere caratterizzate tramite i parametri orbitali kepleriani:
\begin{itemize}
\item il semiasse maggiore $a$;
\item l'eccentricità $e$, dove $e=0$ indica un orbita circolare, mentre $e\to 1$ descrive un'orbita progressivamente più schiacciata;
\item l'inclinazione $i\in[0,\pi]$ del piano dell'orbita, in genere espressa rispetto al piano dell'orbita della Terra, ma in questo caso espressa rispetto al piano xy del sistema di riferimento, con $i<\frac{\pi}{2}$ indicante un'orbita prograda e $i>\frac{\pi}{2}$ indicante un'orbita retrograda;
\item la longitudine del nodo ascendente $\Omega$;
\item l'argomento del periapside $\omega$.
\end{itemize}
La posizione del corpo nell'orbita è identificata dall'anomalia. Più precisamente, la distanza angolare dal periapside è detta anomalia vera e indicata con $\nu$. \\
Si definisce la distanza radiale $r$ dal fuoco occupato e l'argomento di latitudine $u=\omega+\nu$, cosicché le coordinate cartesiane del corpo siano date da:
\begin{equation}
\begin{pmatrix}x \\ y \\ z\end{pmatrix}=R_z(\Omega)R_x(i)R_z(u)\begin{pmatrix}r \\ 0 \\ 0\end{pmatrix}=\begin{pmatrix}r\bigl(\cos(\Omega)\cos(u)-\sin(\Omega)\sin(u)\cos(i)\bigr) \\ r\bigl(\sin(\Omega)\cos(u)+\cos(\Omega)\sin(u)\cos(i)\bigr) \\ r\sin(u)\sin(i)\end{pmatrix}
\end{equation}
Per studiare l'evoluzione della posizione nel tempo si introducono i concetti di anomalia media $M$ e anomalia eccentrica $E$. \\
La prima avanza linearmente nel tempo in base al moto medio $n$ e al tempo di periapside $\tau$:
\begin{equation}
M=n(t-\tau)
\end{equation}
Il moto medio è definito dal periodo dell'orbita come $n=\frac{2\pi}{T}$ ed è ricavabile dal semiasse maggiore e dal parametro gravitazionale $\mu=GM$ del corpo primario (in questo caso $G$ è la costante di gravitazione universale e $M$ è la massa del corpo primario):
\begin{equation}
n=\sqrt{\frac{\mu}{a^3}}
\end{equation}
Una volta determinata l'anomalia media, si risale all'anomalia eccentrica esprimendo gli angoli in radianti e risolvendo l'equazione di Keplero tramite metodi numerici:
\begin{equation}
M=E-e\sin(E)
\end{equation}
In questo caso si è scelto di utilizzare il metodo di Newton, ponendo $f(x)=x-e\sin(x)-M$, $f^\prime(x)=1-e\cos(x)$ e $x_0=M$. \\
Dall'anomalia eccentrica è possibile passare all'anomalia vera. Una formula numericamente stabile richiede di calcolare il valore intermedio $\beta$:
\begin{gather}
\beta=\frac{e}{1+\sqrt{1-e^2}} \\
\nu=E+2\arctan\left(\frac{\beta\sin(E)}{1-\beta\cos(E)}\right)
\end{gather}
Infine, la distanza radiale è legata all'anomalia eccentrica da:
\begin{equation}
r=a(1-e\cos(E))
\end{equation}

\chapter{Applicazione del filtro}

\section{Sfide sul tracciamento di angoli}

Per via della natura del problema, si vuole memorizzare quantità angolari sia nel vettore di stato che nel vettore di misurazione. Tuttavia, fare ciò senza dovuti accorgimenti introdurrebbe forti discontinuità nelle equazioni che descrivono il sistema. Si noti, ad esempio, che all'avanzare dell'anomalia, questa passa da $\nu\approx2\pi$ a $\nu\approx0$. In aggiunta, una stima $\hat{\nu}\approx2\pi$ affiancata ad una misurazione $\nu\approx0$ è nella realtà da considerarsi valida, ma dal punto di vista del filtro apparirebbe come una grossa discrepanza.

Per affrontare questo problema, si decide di non memorizzare direttamente gli angoli, ma bensì di rappresentarli nelle loro componenti sulla circonferenza unitaria, ossia la coppia $\bigl(cos(\cdot),sin(\cdot)\bigr)$. Questo approccio è analogo alla rappresentazione di rotazioni in tre dimensioni attraverso l'uso di quaternioni e permette di rimuovere le discontinuità di cui si è parlato.

In figura~\ref{fig:angle-method-comparison} si mostra l'efficacia di questa soluzione applicata alla simulazione di un corpo in moto circolare uniforme.

Tuttavia restano delle limitazioni, dovute al fatto che questa trasformazione è pressoché lineare solo per piccoli errori sugli angoli. Ne segue, ad esempio, che la media fra due coppie di componenti possa non corrispondere esattamente alla media dei due rispettivi angoli, oppure possa non risiedere precisamente sulla circonferenza unitaria.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/angle-method-comparison.png}
    \caption{Confronto dei metodi per la stima di angoli.}
    \label{fig:angle-method-comparison}
    \vspace{2\baselineskip}
\end{figure}

\section{Scelta dei vettori di stato e di misurazione}

\begin{equation}
z=\begin{pmatrix}
\cos(\alpha) \\
\sin(\alpha) \\
\cos(\delta) \\
\sin(\delta) \\
V
\end{pmatrix}
\end{equation}
\begin{equation}
x=\begin{pmatrix}
\cos(M) \\
\sin(M) \\
a \\
e \\
\cos(i) \\
\sin(i) \\
\cos(\Omega) \\
\sin(\Omega) \\
\cos(\omega) \\
\sin(\omega) \\
n \\
H \\
G
\end{pmatrix}
\end{equation}

\chapter{Contribution}

You may also put some code snippet (which is NOT float by default), eg: \cref{lst:random-code}.

\lstinputlisting[float,language=Java,label={lst:random-code}]{listings/HelloWorld.java}

\section{Fancy formulas here}

%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\backmatter

% \nocite{*} % Remove this as soon as you have the first citation

% \bibliographystyle{alpha}
\bibliographystyle{ieeetr}
\bibliography{bibliography}

\begin{acknowledgements} % this is optional
Optional. Max 1 page.
\end{acknowledgements}

\end{document}
